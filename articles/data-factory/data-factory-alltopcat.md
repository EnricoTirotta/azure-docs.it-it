---
title: Tutti gli argomenti per il servizio Azure Data Factory | Microsoft Docs
description: Tabella di tutti gli argomenti per il servizio Azure Data Factory presenti in http://azure.microsoft.com/documentation/articles/, con titolo e descrizione.
services: data-factory
documentationcenter: ''
author: spelluru
manager: jhubbard
editor: MightyPen

ms.service: data-factory
ms.workload: data-factory
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 10/05/2016
ms.author: spelluru

---
# <a name="all-topics-for-azure-data-factory-service"></a>Tutti gli argomenti per il servizio Azure Data Factory
Questo argomento fornisce un elenco di tutti gli argomenti applicabili direttamente al servizio **Azure Data Factory** . È possibile cercare la pagina Web per le parole chiave utilizzando **Ctrl+F**, per trovare gli argomenti di interesse corrente.

## <a name="new"></a>Nuovo
| &nbsp; | Titolo | Descrizione |
| ---:|:--- |:--- |
| 1 |[Spostare i dati da Amazon Redshift usando Azure Data Factory](data-factory-amazon-redshift-connector.md) |Informazioni su come spostare i dati da origini Amazon Redshift usando Azure Data Factory. |
| 2 |[Spostare i dati dal servizio di archiviazione semplice di Amazon usando Data Factory di Azure](data-factory-amazon-simple-storage-service-connector.md) |Informazioni su come spostare i dati dal servizio di archiviazione semplice di Amazon (S3) usando Data Factory di Azure. |
| 3 |[Copia guidata di Azure Data Factory](data-factory-azure-copy-wizard.md) |Informazioni su come usare Copia guidata di Azure Data Factory per copiare i dati da origini dati supportate nei sink. |
| 4 |[Esercitazione: Creare la prima data factory di Azure usando l'API REST di Data Factory](data-factory-build-your-first-pipeline-using-rest-api.md) |In questa esercitazione viene creata una pipeline di esempio di Azure Data Factory usando l'API REST di Data Factory. |
| 5 |[Esercitazione: Creare una pipeline con l'attività di copia usando l'API .NET](data-factory-copy-activity-tutorial-using-dotnet-api.md) |In questa esercitazione viene creata una pipeline di Azure Data Factory con un'attività di copia usando l'API .NET. |
| 6 |[Esercitazione: Creare una pipeline con l'attività di copia usando l'API REST](data-factory-copy-activity-tutorial-using-rest-api.md) |In questa esercitazione viene creata una pipeline di Azure Data Factory con un'attività di copia usando l'API REST. |
| 7 |[Copia guidata di data factory](data-factory-copy-wizard.md) |Informazioni su come usare Copia guidata di Data Factory per copiare i dati da origini dati supportate nei sink. |
| 8 |[Gateway di gestione dati](data-factory-data-management-gateway.md) |Configurare un gateway dati per spostare dati tra origini locali e il cloud. Usare Gateway di gestione dati in Azure Data Factory per spostare dati. |
| 9 |[Spostare dati da un database Cassandra locale mediante Azure Data Factory](data-factory-onprem-cassandra-connector.md) |Informazioni su come spostare dati da un database Cassandra locale mediante Azure Data Factory. |
| 10 |[Spostare i dati da MongoDB con Azure Data Factory](data-factory-on-premises-mongodb-connector.md) |Informazioni su come spostare i dati dal database di MongoDB con Azure Data Factory. |
| 11 |[Spostare dati da Salesforce usando Azure Data Factory](data-factory-salesforce-connector.md) |Informazioni su come spostare dati da Salesforce usando Azure Data Factory. |

## <a name="updated-articles,-data-factory"></a>Articoli aggiornati, Data Factory
Questa sezione elenca gli articoli aggiornati di recente, in cui l'aggiornamento è stato importante o significativo. Per ogni articolo aggiornato, viene visualizzato un frammento di codice del testo di markdown aggiunto. Gli articoli sono stati aggiornati nell'intervallo di date dal **22-08-2016** al **05-10-2016**.

| &nbsp; | Articolo | Testo aggiornato, frammento di codice | Data aggiornamento |
| ---:|:--- |:--- |:--- |
| 12 |[Data factory di Azure: Log delle modifiche dell'API .NET](data-factory-api-change-log.md) |In questo articolo vengono fornite informazioni sulle modifiche apportate all'SDK di Azure Data Factory in una versione specifica. La versione più recente del pacchetto NuGet per Azure Data Factory è disponibile qui (https://www.nuget.org/packages/Microsoft.Azure.Management.DataFactories) ** Versione 4.11.0** Aggiunte di funzionalità: / Sono stati aggiunti i seguenti tipi di servizi collegati:  -  OnPremisesMongoDbLinkedService (https://msdn.microsoft.com/library/mt765129.aspx)    -  AmazonRedshiftLinkedService (https://msdn.microsoft.com/library/mt765121.aspx)   -  AwsAccessKeyLinkedService (https://msdn.microsoft.com/library/mt765144.aspx) / Sono stati aggiunti i seguenti tipi di set di dati:  -  MongoDbCollectionDataset (https://msdn.microsoft.com/library/mt765145.aspx)  -  AmazonS3Dataset (https://msdn.microsoft.com/library/mt765112.aspx) / Sono stati aggiunti i seguenti tipi di origine dati per le attività di copia:    -  MongoDbSource (https://msdn.microsoft.com/en-US/library/mt765123.aspx) ** Versione 4.10.0** / A TextFormat sono state aggiunte le proprietà facoltative seguenti:    -  Ski |2016-09-22 |
| 13 |[Spostare dati da e verso il BLOB di Azure mediante Data factory di Azure](data-factory-azure-blob-connector.md) |/  copyBehavior  /  Definisce il comportamento di copia quando l'origine è BlobSource o FileSystem.  /  **PreserveHierarchy:** mantiene la gerarchia dei file nella cartella di destinazione. Il percorso relativo del file di origine nella cartella di origine è identico al percorso relativo del file di destinazione nella cartella di destinazione..br/..br/.**FlattenHierarchy:** tutti i file della cartella di origine si trovano nel primo livello della cartella di destinazione. Il nome dei file di destinazione viene generato automaticamente. .br/..br/.**MergeFiles: (valore predefinito)** unisce tutti i file della cartella di origine in un solo file. Se viene specificato il nome file/BLOB, il nome file unito sarà il nome specificato. In caso contrario, sarà il nome file generato automaticamente.  /  No  /  **BlobSource** supporta anche le due proprietà seguenti per la compatibilità con le versioni precedenti. / **treatEmptyAsNull**: specifica se considerare una stringa vuota o Null come valore Null. / **skipHeaderLineCount**: specifica il numero di righe che devono essere ignorate. È applicabile solo quando il set di dati di input usa TextFormat. Analogamente, **BlobSink** supporta |2016-09-28 |
| 14 |[Creare pipeline predittive con le attività di Azure Machine Learning](data-factory-azure-ml-batch-execution-activity.md) |** Il servizio Web richiede più input** Se il servizio Web accetta più input, usare la proprietà **webServiceInputs** invece di **webServiceInput**. Includere anche i set di dati a cui **webServiceInputs** fa riferimento negli **input** dell'attività. Nell'esperimento di Machine Learning di Azure,le porte e i parametri globali di input e output del servizio Web hanno nomi predefiniti ("input1", "input2") che è possibile personalizzare. I nomi scelti per le impostazioni webServiceInputs, webServiceOutputs e globalParameters devono corrispondere esattamente ai nomi negli esperimenti. Per verificare il mapping previsto, è possibile visualizzare il payload della richiesta di esempio nella pagina della Guida relativa all'esecuzione in batch per l'endpoint di Machine Learning di Azure.    {       "name": "PredictivePipeline",       "properties": {             "description": "use AzureML model",             "activities":  {                "name": "MLActivity",               "type": "AzureMLBatchExecution",                "description": "prediction analysis on batch input",                "inputs":  {                    "name": "inputDataset1"                 }, {                    "name": "inputDatase |2016-09-13 |
| 15 |[Guida alle prestazioni dell'attività di copia e all'ottimizzazione](data-factory-copy-activity-performance.md) |1. **Stabilire una baseline**. Durante la fase di sviluppo, testare la pipeline usando l'attività di copia su un campione di dati rappresentativo. È possibile usare il modello di sezionamento di Data Factory (data-factory-scheduling-and-execution.md time-series-datasets-and-data-slices) per limitare la quantità di dati usati.  Per raccogliere le caratteristiche relative a prestazioni e tempo di esecuzione è possibile usare l' **app di monitoraggio e gestione**. Scegliere **Monitoraggio e gestione** nella home page di Data Factory. Nella visualizzazione albero scegliere il **set di dati di output**. Nell'elenco **Activity Windows** (Finestre attività) scegliere l'esecuzione dell'attività di copia. **Activity Windows** (Finestre attività) riporta la durata dell'attività di copia e le dimensioni dei dati copiati. La velocità effettiva è elencata in **Activity Window Explorer**(Esplora finestre attività). Per altre informazioni sull'app, vedere Monitorare e gestire le pipeline di Azure Data Factory con la nuova app di monitoraggio e gestione (data-factory-monitor-manage-app.md).  ! Dettagli esecuzione attività (./media/data-factory-copy-activity-performance/mmapp-activity-run-details.pn |2016-09-27 |
| 16 |[Esercitazione: Creare una pipeline con l’attività Copia utilizzando Visual Studio](data-factory-copy-activity-tutorial-using-visual-studio.md) |Tenere presente quanto segue: - L'oggetto **type** del set di dati è impostato su **AzureBlob**.     - L'oggetto **linkedServiceName** è impostato su **AzureStorageLinkedService**. Questo servizio collegato è stato creato nel passaggio 2.     - L'oggetto **folderPath** è impostato sul contenitore **adftutorial**. È anche possibile specificare il nome di un BLOB all'interno della cartella usando la proprietà **fileName** . Poiché non si specifica il nome del BLOB, i dati da tutti i BLOB nel contenitore sono considerati come dati di input.    - L'oggetto **type** di format è impostato su **TextFormat**  - Nel file di testo sono presenti due campi ΓÇô **FirstName** e **LastName** ΓÇô separati da una virgola (**columnDelimiter**)     - L'oggetto **availability** è impostato su **hourly** (**frequency** è impostato su **hour** e **interval** è impostato su **1**). Quindi, il servizio Data Factory cerca i dati di input ogni ora nella cartella radice del contenitore BLOB**adftutorial**specificato.  Se non si specifica **fileName** per un set di dati di **input**, tutti i file e i BLOB della cartella di input **folderPath** vengono considerati input. |2016-09-29 |
| 17 |[Creazione, monitoraggio e gestione delle istanze di Data factory di Azure mediante Data Factory .NET SDK](data-factory-create-data-factories-programmatically.md) |Annotare l'ID dell'applicazione e la password (segreto client) e usarli nella procedura dettagliata. ** Ottenere l'ID sottoscrizione e l'ID tenant di Azure** Per installare la versione più recente di Azure PowerShell nel computer, seguire le istruzioni disponibili nell'articolo Come installare e configurare Azure PowerShell (../powershell-install-configure.md). 1. Aprire Azure PowerShell ed eseguire il comando seguente. 2. Eseguire il comando seguente e immettere il nome utente e la password usati per accedere al portale di Azure.         Login-AzureRmAccount    Se si dispone di una sola sottoscrizione di Azure associata a questo account, non è necessario eseguire i due passaggi seguenti. 3. Eseguire il comando seguente per visualizzare tutte le sottoscrizioni per l'account.       Get-AzureRmSubscription 4. Eseguire il comando seguente per selezionare la sottoscrizione da usare. Sostituire **NameOfAzureSubscription** con il nome della sottoscrizione di Azure.       Get-AzureRmSubscription -SubscriptionName NameOfAzureSubscription  /  Set-AzureRmCo |2016-09-14 |
| 18 |[Pipeline e attività in Azure Data Factory](data-factory-create-pipelines.md) |,       "start": "2016-07-12T00:00:00Z",    "end": "2016-07-13T00:00:00Z"       }     } Tenere presente quanto segue: / Nella sezione delle attività esiste una sola attività con l'oggetto **type** impostato su **Copy**. / L'input per l'attività è impostato su **InputDataset** e l'output è impostato su **OutputDataset**. / Nella sezione **typeProperties** vengono specificati **BlobSource** come tipo di origine e **SqlSink** come tipo di sink. Per la procedura completa di creazione di questa pipeline, vedere Esercitazione: copiare i dati dall'archivio BLOB al database SQL (data-factory-copy-data-from-azure-blob-storage-to-sql-database.md). ** Esempio di una pipeline di trasformazione** In questa pipeline di esempio è presente un'attività di tipo **HDInsightHive** nella sezione **attività**. In questo esempio, l'attività Hive di HDInsight (data-factory-hive-activity.md) trasforma i dati da un archivio BLOB di Azure tramite l'esecuzione di un file di script Hive in un cluster Hadoop di HDInsight.  {     "name": "TransformPipeline",    "p |2016-09-27 |
| 19 |[Trasformare i dati in Azure Data Factory](data-factory-data-transformation-activities.md) |Data Factory supporta le seguenti attività di trasformazione dei dati che possono essere aggiunte a pipeline (data-factory-create-pipelines.md) singolarmente o con un'altra attività concatenata. .  AZURE.NOTE  Per la procedura dettagliata, vedere l'articolo Creare una pipeline con la trasformazione Hive (data-factory-build-your-first-pipeline.md). ** Attività Hive di HDInsight** L'attività Hive di HDInsight in una pipeline di Data factory esegue query Hive sul proprio cluster HDInsight o sul cluster HDInsight su richiesta basato su Windows o Linux. Vedere l'articolo Attività Hive (data-factory-hive-activity.md) per i dettagli. ** Attività Pig di HDInsight** L'attività Pig di HDInsight in una pipeline di Data factory esegue query Pig sul proprio cluster HDInsight o sul cluster HDInsight su richiesta basato su Windows o Linux. Vedere l'articolo Attività Pig (data-factory-pig-activity.md) per i dettagli. ** Attività MapReduce di HDInsight** L'attività HDInsight MapReduce in una pipeline di Data Factory esegue i programmi di MapReduce nei cluster HDInsight personalizzati o su richiesta basati su Windows/Linux. |2016-09-26 |
| 20 |[Pianificazione ed esecuzione con Data Factory](data-factory-scheduling-and-execution.md) |CopyActivity2 viene eseguita solo se l'esecuzione di CopyActivity1 è riuscita e Dataset2 è disponibile. Ecco la pipeline JSON di esempio:  {       "name": "ChainActivities",    "properties": {           "description": "Run activities in sequence",      "activities":       {       "type": "Copy",     "typeProperties": {     "source": {     "type": "BlobSource"    },      "sink": {       "type": "BlobSink",     "copyBehavior": "PreserveHierarchy",    "writeBatchSize": 0,    "writeBatchTimeout": "00:00:00"     }       },      "inputs":       {       "name": "Dataset1"      }       ,       "outputs":      {       "name": "Dataset2"      }       ,       "policy": {     "timeout": "01:00:00"       },      "scheduler": {      "frequency": "Hour",    "interval": 1       },      "name": "CopyFromBlob1ToBlob2",     "description": "Copy data from a blob to another"       },      {       "type": "Copy",     "typeProperties": {     "source": {     "type": "BlobSource"    },      "sink": {       "type": "BlobSink",     "writeBatchSize": 0,    "writeBatchTimeout": "00:00:00"     }       },      "in |2016-09-28 |

## <a name="tutorials"></a>Esercitazioni
| &nbsp; | Titolo | Descrizione |
| ---:|:--- |:--- |
| 21 |[Esercitazione: Creare la prima pipeline per elaborare i dati usando il cluster Hadoop](data-factory-build-your-first-pipeline.md) |Questa esercitazione di Azure Data Factory illustra come creare e pianificare una data factory che elabora i dati usando uno script Hive in un cluster Hadoop. |
| 22 |[Esercitazione: Creare la prima data factory di Azure usando il modello di Azure Resource Manager](data-factory-build-your-first-pipeline-using-arm.md) |In questa esercitazione si crea una pipeline di esempio di Azure Data Factory usando il modello di Azure Resource Manager. |
| 23 |[Esercitazione: Creare la prima data factory di Azure con il portale di Azure](data-factory-build-your-first-pipeline-using-editor.md) |In questa esercitazione viene creata una pipeline di esempio di Azure Data Factory usando l'editor di Data Factory nel portale di Azure. |
| 24 |[Esercitazione: Creare la prima data factory di Azure con Azure PowerShell](data-factory-build-your-first-pipeline-using-powershell.md) |In questa esercitazione viene creata una pipeline di esempio di Azure Data Factory usando Azure PowerShell. |
| 25 |[Esercitazione: Creare la prima data factory di Azure con Microsoft Visual Studio](data-factory-build-your-first-pipeline-using-vs.md) |In questa esercitazione viene creata una pipeline di esempio di Azure Data Factory usando Visual Studio. |
| 26 |[Esercitazione: Creare una pipeline con l'attività di copia usando il portale di Azure](data-factory-copy-activity-tutorial-using-azure-portal.md) |In questa esercitazione viene creata una pipeline di esempio di Azure Data Factory con un'attività di copia usando l'editor di Data Factory nel portale di Azure. |
| 27 |[Esercitazione: Creare una pipeline con l’attività Copia utilizzando Azure PowerShell](data-factory-copy-activity-tutorial-using-powershell.md) |In questa esercitazione viene creata una pipeline di Azure Data Factory con un'attività di copia usando Azure PowerShell. |
| 28 |[Esercitazione: Creare una pipeline con l’attività Copia utilizzando Visual Studio](data-factory-copy-activity-tutorial-using-visual-studio.md) |In questa esercitazione viene creata una pipeline di Azure Data Factory con un'attività di copia usando Visual Studio. |
| 29 |[Copiare dati da un archivio BLOB al database SQL usando Data Factory](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md) |Questa esercitazione illustra come usare l'attività di copia in una pipeline di Azure Data Factory per copiare i dati da un archivio BLOB a un database SQL. |
| 30 |[Esercitazione: Creare una pipeline con l'attività di copia usando la Copia guidata di Data Factory](data-factory-copy-data-wizard-tutorial.md) |In questa esercitazione viene creata una pipeline di Azure Data Factory con un'attività di copia usando la Copia guidata supportata da Data Factory. |

## <a name="data-movement"></a>Spostamento dei dati
| &nbsp; | Titolo | Descrizione |
| ---:|:--- |:--- |
| 31 |[Spostare dati da e verso il BLOB di Azure mediante Data factory di Azure](data-factory-azure-blob-connector.md) |Informazioni su come copiare dati BLOB in Azure Data Factory. Usare l'esempio: Come copiare dati da e verso l'archivio BLOB di Azure e il database SQL di Azure. |
| 32 |[Spostare dati da e in Archivio Azure Data Lake con Data factory di Azure](data-factory-azure-datalake-connector.md) |Informazioni su come spostare i dati da e in Archivio Azure Data Lake con Azure Data Factory |
| 33 |[Spostare dati da e verso DocumentDB mediante Data factory di Azure](data-factory-azure-documentdb-connector.md) |Informazioni su come spostare i dati da e verso la raccolta di Azure DocumentDB mediante Data factory di Azure |
| 34 |[Spostare dati da e nel database SQL di Azure con Data factory di Azure](data-factory-azure-sql-connector.md) |Informazioni su come spostare i dati da e verso il database SQL di Azure mediante Data factory di Azure. |
| 35 |[Spostare dati da e verso Azure SQL Data Warehouse mediante Data factory di Azure](data-factory-azure-sql-data-warehouse-connector.md) |Informazioni su come spostare i dati da e verso Azure SQL Data Warehouse mediante Data factory di Azure |
| 36 |[Spostare dati da e verso le tabelle di Azure mediante Data factory di Azure](data-factory-azure-table-connector.md) |Informazioni su come spostare i dati da e verso l'archiviazione tabelle di Azure mediante Data factory di Azure. |
| 37 |[Guida alle prestazioni dell'attività di copia e all'ottimizzazione](data-factory-copy-activity-performance.md) |Informazioni sui fattori principali che influiscono sulle prestazioni dello spostamento di dati in Azure Data Factory quando si usa l'attività di copia. |
| 38 |[Spostare dati con l'attività di copia](data-factory-data-movement-activities.md) |Informazioni sullo spostamento di dati in pipeline di Data Factory: migrazione di dati tra archivi cloud e tra archivi locali e cloud. Usare l'attività di copia. |
| 39 |[Note sulla versione di Gateway di gestione dati](data-factory-gateway-release-notes.md) |Note sulla versione di Gateway di gestione dati |
| 40 |[Spostare dati da HDFS locale con Data factory di Azure](data-factory-hdfs-connector.md) |Informazioni su come spostare dati da un HDFS locale con Azure Data Factory. |
| 41 |[Monitorare e gestire le pipeline di Azure Data Factory con la nuova app di monitoraggio e gestione](data-factory-monitor-manage-app.md) |Informazioni sull'uso dell'app di monitoraggio e gestione per monitorare e gestire le data factory e le pipeline di Azure. |
| 42 |[Spostare dati tra origini locali e il cloud con Gateway di gestione dati](data-factory-move-data-between-onprem-and-cloud.md) |Configurare un gateway dati per spostare dati tra origini locali e il cloud. Usare Gateway di gestione dati in Azure Data Factory per spostare dati. |
| 43 |[Spostare i dati da un'origine OData usando Azure Data Factory](data-factory-odata-connector.md) |Informazioni su come spostare i dati da origini OData usando Azure Data Factory. |
| 44 |[Spostare dati da archivi dati ODBC con Data factory di Azure](data-factory-odbc-connector.md) |Informazioni su come spostare dati da archivi dati ODBC con Azure Data Factory. |
| 45 |[Spostare i dati da DB2 mediante Data factory di Azure](data-factory-onprem-db2-connector.md) |Informazioni su come spostare i dati dal database di DB2 mediante Data factory di Azure |
| 46 |[Spostare i dati da e in un file system locale usando Azure Data Factory](data-factory-onprem-file-system-connector.md) |Informazioni su come spostare i dati da e in un file system locale usando Azure Data Factory. |
| 47 |[Spostare i dati da MySQL mediante Data factory di Azure](data-factory-onprem-mysql-connector.md) |Informazioni su come spostare i dati dal database di MySQL mediante Data factory di Azure. |
| 48 |[Spostare i dati da/verso Oracle locale con Azure Data Factory](data-factory-onprem-oracle-connector.md) |Informazioni su come spostare i dati da e verso database Oracle in locale mediante Data factory di Azure. |
| 49 |[Spostare i dati da PostgreSQL mediante Data factory di Azure](data-factory-onprem-postgresql-connector.md) |Informazioni su come spostare i dati dal database di PostgreSQL mediante Data factory di Azure. |
| 50 |[Spostare i dati da Sybase utilizzando Data factory di Azure](data-factory-onprem-sybase-connector.md) |Informazioni su come spostare i dati dal Database di Sybase utilizzando Data factory di Azure. |
| 51 |[Spostare i dati da Teradata utilizzando Data factory di Azure](data-factory-onprem-teradata-connector.md) |Informazioni su come il connettore Teradata per il servizio Data factory consente di spostare dati dal database Teradata |
| 52 |[Spostare i dati da e verso SQL Server locale o in IaaS (VM di Azure) utilizzando Data factory di Azure](data-factory-sqlserver-connector.md) |Informazioni su come spostare i dati da/verso il database di SQL Server locale o in una VM di Azure utilizzando Data factory di Azure. |
| 53 |[Spostare i dati da un'origine tabella Web usando Azure Data Factory](data-factory-web-table-connector.md) |Informazioni su come spostare dati da una tabella locale a una pagina Web con Azure Data Factory. |

## <a name="data-transformation"></a>Trasformazione dei dati
| &nbsp; | Titolo | Descrizione |
| ---:|:--- |:--- |
| 54 |[Creare pipeline predittive con le attività di Azure Machine Learning](data-factory-azure-ml-batch-execution-activity.md) |Illustra come creare pipeline predittive usando Data factory di Azure e Azure Machine Learning |
| 55 |[Servizi collegati di calcolo](data-factory-compute-linked-services.md) |Informazioni sugli ambienti di calcolo che è possibile utilizzare nelle pipeline di Data Factory di Azure per trasformare/elaborare i dati.. |
| 56 |[Elaborazione di fogli dati su larga scala con Data Factory e Batch](data-factory-data-processing-using-batch.md) |Descrive come elaborare elevate quantità di dati in una pipeline di Data factory di Azure usando la funzionalità di elaborazione parallela di Azure Batch. |
| 57 |[Trasformare i dati in Azure Data Factory](data-factory-data-transformation-activities.md) |Informazioni su come trasformare o elaborare i dati in Azure Data Factory con Hadoop, Machine Learning o Azure Data Lake Analytics. |
| 58 |[Attività di Hadoop Streaming](data-factory-hadoop-streaming-activity.md) |Informazioni su come usare l'attività di Hadoop Streaming in una Data factory di Azure per eseguire i programmi di Hadoop Streaming in un cluster HDInsight personalizzato o on demand. |
| 59 |[Attività Hive](data-factory-hive-activity.md) |Informazioni su come usare l'attività Hive in una data factory di Azure per eseguire query Hive in un cluster HDInsight su richiesta o nel proprio cluster HDInsight. |
| 60 |[Richiamare i programmi MapReduce da Data factory](data-factory-map-reduce.md) |Informazioni su come elaborare i dati eseguendo programmi MapReduce in un cluster Azure HDInsight da un'istanza di Data factory di Azure. |
| 61 |[Attività di Pig](data-factory-pig-activity.md) |Informazioni su come usare l'attività Pig in una data factory di Azure per eseguire query Pig in un cluster HDInsight su richiesta o nel proprio cluster HDInsight. |
| 62 |[Chiamare i programmi Spark da Data Factory](data-factory-spark.md) |È possibile chiamare i programmi Spark da una data factory di Azure usando l'attività MapReduce. |
| 63 |[Attività di stored procedure di SQL Server](data-factory-stored-proc-activity.md) |Informazioni sull'uso dell'attività di stored procedure di SQL Server da una pipeline di Data factory per richiamare una stored procedure in un database SQL di Azure o in Azure SQL Data Warehouse. |
| 64 |[Usare attività personalizzate in una pipeline di Data factory di Azure](data-factory-use-custom-activities.md) |Informazioni su come creare attività personalizzate e usarle in una pipeline di Azure Data Factory. |
| 65 |[Eseguire script U-SQL in Azure Data Lake Analytics da Data factory di Azure](data-factory-usql-activity.md) |Informazioni su come elaborare i dati eseguendo gli script U-SQL nel servizio di calcolo di Azure Data Lake Analytics. |

## <a name="samples"></a>Esempi
| &nbsp; | Titolo | Descrizione |
| ---:|:--- |:--- |
| 66 |[Data factory di Azure - Esempi](data-factory-samples.md) |Fornisce informazioni dettagliate sugli esempi inclusi nel servizio Data factory di Azure. |

## <a name="use-cases"></a>Casi di utilizzo
| &nbsp; | Titolo | Descrizione |
| ---:|:--- |:--- |
| 67 |[Casi di studio sui clienti](data-factory-customer-case-studies.md) |Scopri come alcuni dei nostri clienti utilizzano Data Factory di Azure. |
| 68 |[Caso di utilizzo - Profilo clienti](data-factory-customer-profiling-usecase.md) |Informazioni su come utilizzare Data Factory di Azure per creare un flusso di lavoro basato sui dati (pipeline) per analizzare il profilo dei clienti di società di giochi. |
| 69 |[Caso d'uso - Consigli sui prodotti](data-factory-product-reco-usecase.md) |Descrizione di un caso d'uso implementato usando Data factory di Azure e altri servizi. |

## <a name="monitor-and-manage"></a>Monitoraggio e gestione
| &nbsp; | Titolo | Descrizione |
| ---:|:--- |:--- |
| 70 |[Monitorare e gestire le pipeline di Data factory di Azure](data-factory-monitor-manage-pipelines.md) |Sono disponibili informazioni sull'uso del portale di Azure e di Azure PowerShell per monitorare e gestire le data factory di Azure e le pipeline create. |

## <a name="sdk"></a>SDK
| &nbsp; | Titolo | Descrizione |
| ---:|:--- |:--- |
| 71 |[Data factory di Azure: Log delle modifiche dell'API .NET](data-factory-api-change-log.md) |Vengono descritte le modifiche rilevanti, le aggiunte di funzionalità, le correzioni di bug e così via in una versione specifica dell'API .NET per Azure Data Factory. |
| 72 |[Creazione, monitoraggio e gestione delle istanze di Data factory di Azure mediante Data Factory .NET SDK](data-factory-create-data-factories-programmatically.md) |Informazioni su come creare, monitorare e gestire a livello di codice le istanze di Data factory di Azure usando Data Factory SDK. |
| 73 |[Guida di riferimento per gli sviluppatori di Data factory di Azure](data-factory-sdks.md) |Informazioni sui vari modi per creare, monitorare e gestire data factory di Azure |

## <a name="miscellaneous"></a>Miscellaneous
| &nbsp; | Titolo | Descrizione |
| ---:|:--- |:--- |
| 74 |[Data factory di Azure - Domande frequenti](data-factory-faq.md) |Domande frequenti su Azure Data Factory. |
| 75 |[Azure Data Factory - Funzioni e variabili di sistema](data-factory-functions-variables.md) |Fornisce un elenco delle funzioni e delle variabili di sistema di Azure Data Factory |
| 76 |[Data factory di Azure - Regole di denominazione](data-factory-naming-rules.md) |Descrive le regole di denominazione per le entità di Data factory. |
| 77 |[Risolvere i problemi di Data factory](data-factory-troubleshoot.md) |Informazioni su come risolvere i problemi relativi all'uso di Data factory di Azure. |

<!--HONumber=Oct16_HO2-->


